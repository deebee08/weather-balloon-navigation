{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a1d6230",
   "metadata": {},
   "source": [
    "# Imitation Learning\n",
    "\n",
    "## Objective: Fly balloon as far as possible.\n",
    "\n",
    "Imitation learning is a paradigm where an agent learns to mimic an **expert's behavior** instead of learning purely from trial-and-error rewards (e.g., reinforcement learning). \n",
    "\n",
    "The simplest common approach is called \"**behavior cloning**\" (BC). It treats the expert's demonstrations as training data samples; train a policy using the expert's input(state) and output(action) pairs via supervised learning. However, BC often fails because the policy is only trained on states the expert visited, so if it ever drifts off that trajectory, it encounters states that the policy doesn't know what to do.\n",
    "\n",
    "**[DAgger (Dataset Aggregation)](https://arxiv.org/pdf/1011.0686)** is an interactive imitation learning algorithm that deals with the issue above. The key idea is to keep utilizing the expert's knowledge during training and gradually expand the training data to include those \"off-course\" states. Instead of training the policy by cloning the expert's behavior, DAgger iteratively refines the policy:\n",
    "\n",
    "1. After initial training on expert data (behavior cloning), execute the policy in the environment. (Policy rollout) \n",
    "2. Store all the states the policy visits (it may include \"bad\" states).\n",
    "3. Query the expert for the correct action in each of those states. (Expert labeling)\n",
    "4. Aggregate these new state-action pairs into the training dataset.\n",
    "5. Update the policy on this expanded dataset (so it knows what to do in new states).\n",
    "6. Repeat this process.\n",
    "\n",
    "Although DAgger requires querying the expert online, it improves behavior cloning by training on a dataset that better resembles the observations the trained policy is likely to encounter. [(Reference)](https://imitation.readthedocs.io/en/latest/algorithms/dagger.html#:~:text=DAgger%20,requires%20querying%20the%20expert%20online)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15726190",
   "metadata": {},
   "source": [
    "## For Colab users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd963850",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Colab users ##\n",
    "\n",
    "# Clone the repository\n",
    "!git clone https://github.com/sdean-group/balloon-outreach.git\n",
    "%cd balloon-outreach\n",
    "!git checkout main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cae82e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Colab users ##\n",
    "\n",
    "# Install required packages\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3034de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Colab users ##\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37b3e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Colab users ##\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/balloon-outreach/')\n",
    "\n",
    "datapath = \"/content/drive/MyDrive/era5_data.nc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c96e357",
   "metadata": {},
   "source": [
    "## For locals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4807257",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'era5_data.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa917c3e",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed7d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from env.balloon_env import BalloonERAEnvironment\n",
    "from agent.mppi_agent import MPPIAgentWithCostFunction, MPPIAgent\n",
    "from utils.learning_util import run_expert_episode, evaluate_policy, train_one_epoch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65b113a",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e817e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds          = xr.open_dataset(datapath, engine=\"netcdf4\")\n",
    "start_time  = dt.datetime(year=2024, month=7, day=1, hour=0, minute=0)\n",
    "\n",
    "\n",
    "#This is Ithaca\n",
    "initial_lat = 42.6\n",
    "initial_lon = -76.5\n",
    "initial_alt = 10.0\n",
    "\n",
    "target_lat = None\n",
    "target_lon = None\n",
    "target_alt = None\n",
    "\n",
    "time_step = 120 #120 seconds\n",
    "objective = 'fly'\n",
    "\n",
    "env = BalloonERAEnvironment(ds=ds,\n",
    "                            start_time=start_time,\n",
    "                            initial_lat=initial_lat,\n",
    "                            initial_lon=initial_lon,\n",
    "                            initial_alt=initial_alt,\n",
    "                            target_lat=target_lat,\n",
    "                            target_lon=target_lon,\n",
    "                            target_alt=target_alt,\n",
    "                            objective=objective,\n",
    "                            dt=time_step,\n",
    "                            viz=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc5c531",
   "metadata": {},
   "source": [
    "## Define policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021b3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_dim=21, hidden_dim=64, output_dim=1):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim*2)\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim*2)\n",
    "        self.fc2 = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Weight initialization\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu')\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight, gain=nn.init.calculate_gain('tanh'))\n",
    "        nn.init.zeros_(self.fc3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, input_dim)\n",
    "        x = self.fc1(x)\n",
    "        x = self.ln1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.ln2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.tanh(x)\n",
    "\n",
    "# Initialize policy network and optimizer\n",
    "policy = PolicyNet()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935c1c3b",
   "metadata": {},
   "source": [
    "## Behavior Cloning to pretrain our policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48034a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for MPPI agent\n",
    "\n",
    "max_steps = int(1440/(time_step/60)) #1 day\n",
    "\n",
    "num_samples=10\n",
    "acc_bounds= (-0.1, 0.1)\n",
    "noise_std = 0.1\n",
    "num_iterations=1\n",
    "temperature=10\n",
    "horizon = 10 \n",
    "\n",
    "# Define expert, run an episode from an expert, and collect its behavior.\n",
    "\n",
    "expert = MPPIAgentWithCostFunction(target_lat=target_lat,\n",
    "                                   target_lon=target_lon,\n",
    "                                   target_alt=target_alt,\n",
    "                                   num_samples=num_samples,\n",
    "                                   acc_bounds= acc_bounds,\n",
    "                                   noise_std=noise_std,\n",
    "                                   num_iterations=num_iterations,\n",
    "                                   temperature=temperature,\n",
    "                                   horizon=horizon,\n",
    "                                   visualize=False,\n",
    "                                   objective=objective)\n",
    "\n",
    "env.wind_field.disable_noise()\n",
    "# env.wind_field.enable_noise(noise_seed=100)\n",
    "expert_total_reward_list = []\n",
    "expert_states_list = []\n",
    "expert_actions_list = []\n",
    "expert_plot_fnms_list = []\n",
    "num_iter = 3\n",
    "for i in range(num_iter):\n",
    "    expert_total_reward, expert_states_np, expert_actions_np, fnm = run_expert_episode(env, expert, max_steps=max_steps, dt=time_step, policy_name=f'expert_{i+1}')\n",
    "    expert_total_reward_list.append(expert_total_reward)\n",
    "    expert_states_list.append(expert_states_np)\n",
    "    expert_actions_list.append(expert_actions_np)\n",
    "    expert_plot_fnms_list.append(fnm)\n",
    "\n",
    "initial_expert_states_np = np.concatenate(expert_states_list)\n",
    "initial_expert_actions_np = np.concatenate(expert_actions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c2fe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename=expert_plot_fnms_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a2f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expert_total_reward_list)\n",
    "expert_avg_total_reward = sum(expert_total_reward_list)/len(expert_total_reward_list)\n",
    "print(f\"Expert Trajectory reward in average: {expert_avg_total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c5415d",
   "metadata": {},
   "source": [
    "### Initial Dataset & DataLoader Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b61f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pytorch tensors\n",
    "states_tensor = torch.from_numpy(initial_expert_states_np).float()               # shape (N, 21)\n",
    "actions_tensor = torch.from_numpy(initial_expert_actions_np).float()             # shape (N, 1)\n",
    "\n",
    "# Construct TensorDataset and DataLoader\n",
    "dataset = TensorDataset(states_tensor, actions_tensor)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e0bf9b",
   "metadata": {},
   "source": [
    "### Pretrain our policy with Behavior Cloning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e530b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train_one_epoch(loader, policy, optimizer, loss_fn)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} — Avg Loss: {avg_loss:.6f} \\n\")\n",
    "\n",
    "# plot the loss curve from initial training (behavior cloning)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(range(1, len(losses)+1), losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Training Loss')\n",
    "plt.title('Behavior Cloning Loss Curve')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e540214",
   "metadata": {},
   "source": [
    "## DAgger Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3379e563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DAgger Iterations using DataLoader ===\n",
    "\n",
    "# Parameters\n",
    "dagger_iterations       = 3\n",
    "episodes_per_iteration  = 1\n",
    "train_epochs_per_iter   = 5\n",
    "batch_size              = 64\n",
    "\n",
    "# Start from initial BC dataset\n",
    "states_np  = initial_expert_states_np.copy()   # (N0, 21)\n",
    "actions_np = initial_expert_actions_np.copy()  # (N0,)\n",
    "\n",
    "plot_fnms_list = []\n",
    "\n",
    "for it in range(dagger_iterations):\n",
    "    new_states = []\n",
    "    new_actions = []\n",
    "\n",
    "    # 1. Collect new data by rolling out current policy\n",
    "    for ep in range(episodes_per_iteration):\n",
    "        start = time.time()\n",
    "        state = env.reset()\n",
    "        for step in range(max_steps):\n",
    "            # Student policy action\n",
    "            state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            action_pred  = policy(state_tensor).item()\n",
    "\n",
    "            # Record state & expert correction\n",
    "            new_states.append(state)\n",
    "            expert_act = expert.select_action(state, env, step)\n",
    "            new_actions.append(expert_act)\n",
    "\n",
    "            # Step environment with student action\n",
    "            state, _, done, info = env.step(action_pred)\n",
    "            if done:\n",
    "                print(f\"\\nEpisode terminated: {info}\")\n",
    "                break\n",
    "        end = time.time()\n",
    "        print(f\"Episode {ep+1}/{episodes_per_iteration} in DAgger Iteration {it+1}/{dagger_iterations} is done. \\n Time: {end-start:.2f} seconds\")\n",
    "    \n",
    "\n",
    "    # 2. Append new data to the NumPy arrays\n",
    "    new_states_np  = np.array(new_states, dtype=np.float32)\n",
    "    new_actions_np = np.array(new_actions, dtype=np.float32)\n",
    "    states_np  = np.concatenate([states_np,  new_states_np], axis=0)\n",
    "    actions_np = np.concatenate([actions_np, new_actions_np], axis=0)\n",
    "\n",
    "    # 3. Rebuild dataset & loader\n",
    "    states_tensor  = torch.from_numpy(states_np).float()\n",
    "    actions_tensor = torch.from_numpy(actions_np).float()\n",
    "    dataset = TensorDataset(states_tensor, actions_tensor)\n",
    "    loader  = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 4. Train on the aggregated dataset\n",
    "    print(f\"\\n--- DAgger Iteration {it+1}/{dagger_iterations} Training ---\")\n",
    "    iter_losses = []\n",
    "    for epoch in range(train_epochs_per_iter):\n",
    "        avg_loss = train_one_epoch(loader, policy, optimizer, loss_fn)\n",
    "        iter_losses.append(avg_loss)\n",
    "        print(f\" Iter {it+1} Epoch {epoch+1}/{train_epochs_per_iter} — Avg Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    # 5. Evaluate policy\n",
    "    env.wind_field.disable_noise()\n",
    "    # env.wind_field.enable_noise(noise_seed=100)\n",
    "    total_reward, plot_fnm = evaluate_policy(\n",
    "        env, \n",
    "        policy, \n",
    "        objective,\n",
    "        max_steps=max_steps, \n",
    "        policy_name=f'DAgger_trained_policy_{it+1}',\n",
    "        expert_avg_total_reward=expert_avg_total_reward\n",
    "    )\n",
    "    plot_fnms_list.append(plot_fnm)\n",
    "    print(f\" After DAgger Iter {it+1}, Total Reward = {total_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa43e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=plot_fnms_list[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "outreach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
